{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f69ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8e23e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b4b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm_dis_sum=pd.read_csv('df_adm_dis_sum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84453845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_adm_dis_sum.readmitted = np.where((df_adm_dis_sum.READMISSION_STATUS=='Readmitted'),1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72383c03",
   "metadata": {},
   "source": [
    "## Apply the word2vec model on the MIMIC III dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(df_adm_dis_sum)):\n",
    "   corpus.append(df_adm_dis_sum.TEXT_AGG[i].split())\n",
    "# I would like to have a 300-dimentional vocabulary for the word2vec embeddings\n",
    "word2vec_paragraph_model = gensim.models.Word2Vec(sentences=corpus,vector_size=300)\n",
    "# Size of vocabulary\n",
    "len(word2vec_paragraph_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786864d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_paragraph_model.save(\"word2vec_paragraph.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17a940",
   "metadata": {},
   "source": [
    "## Sequencing, padding and creating vector matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbd10c",
   "metadata": {},
   "source": [
    "### Tokenizer ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13897d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the maximum length of the list of lists of tokens\n",
    "# max_length=0\n",
    "# for i in range(len(df_adm_dis_sum)):\n",
    "#    length=len(df_adm_dis_sum.TEXT_AGG[i])\n",
    "#    if length>max_length:\n",
    "#       max_length=length\n",
    "# max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d651dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda\\envs\\nlp_course\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The maximum length of the text is way too large (nearly 50000), so I set the max_length to 4000\n",
    "maxlength=4000\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t=Tokenizer()\n",
    "t.fit_on_texts(df_adm_dis_sum.TEXT_AGG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bf4bc",
   "metadata": {},
   "source": [
    "### Sequencing and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3833f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing=t.texts_to_sequences(df_adm_dis_sum.TEXT_AGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6486075",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding=sequence.pad_sequences(sequencing,maxlen=maxlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fec81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('dis_sum_padding',padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae51164",
   "metadata": {},
   "source": [
    "### vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2c2e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the words trained in the word2vec model with their vectors\n",
    "word2vec_paragraph_model = Word2Vec.load(\"word2vec_paragraph.model\")\n",
    "word_vec_dict={}\n",
    "vocab_index=word2vec_paragraph_model.wv.key_to_index\n",
    "for key in vocab_index:\n",
    "  word_vec_dict[key]=word2vec_paragraph_model.wv.get_vector(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b772fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector matrix\n",
    "vocab_size = len(t.word_index) + 1\n",
    "embed_dim=300\n",
    "embed_matrix=np.zeros(shape=(vocab_size,embed_dim),dtype=np.int8) # Create a matrix filled with zeros\n",
    "for word,i in t.word_index.items():\n",
    "  embed_vector=word_vec_dict.get(word)\n",
    "  if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "    embed_matrix[i]=embed_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf15725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embed_matrix.npy', embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52cc8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118556, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c0aad",
   "metadata": {},
   "source": [
    "## Split the dataset to training set (with subsampling) and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcf135c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# One-hot encoding\n",
    "y=keras.utils.to_categorical(df_adm_dis_sum.readmitted,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ae10fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the padding array and the y array to the pre_subsampling set\n",
    "pre_sub=np.concatenate((padding,y),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47f4745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the pre_subsampling training set and the test set\n",
    "train_pre_sub, test=train_test_split(pre_sub,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2981f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the pre-subsampling training set by readmitted and non-readmitted\n",
    "train_readm=[]\n",
    "train_nreadm=[]\n",
    "\n",
    "for i in range(len(train_pre_sub)):\n",
    "    if train_pre_sub[i][-1]==1:\n",
    "        train_readm.append(train_pre_sub[i])\n",
    "    else:\n",
    "        train_nreadm.append(train_pre_sub[i])\n",
    "        \n",
    "train_readm=np.array(train_readm)\n",
    "train_nreadm=np.array(train_nreadm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "254a214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-sample the non-readmitted part and concatenate the two sets into one array\n",
    "random_indices = np.random.choice(train_nreadm.shape[0], size=len(train_readm), replace=False)\n",
    "train_sub=np.concatenate((train_readm,train_nreadm[random_indices, :]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d25f367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_sub[:,0:-2]\n",
    "y_train=train_sub[:,-2:]\n",
    "X_test=test[:,0:-2]\n",
    "y_test=test[:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "df05b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_padding',X_train)\n",
    "np.save('y_train_padding',y_train)\n",
    "np.save('X_test_padding',X_test)\n",
    "np.save('y_test_padding',y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
